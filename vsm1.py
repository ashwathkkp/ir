# -*- coding: utf-8 -*-
"""Copy of IR_Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19k_7mQcvVc6SuguUWkKLtm9jBEBsr_3Q

# general concepts
tf
tf vector -> norm. tf
idf_t = log(N/ df_t)
tf-idf :  wt_term_doc = (1 + log tf_td) * log(N/ df_t)
term - doc incidence
text preprocess - tokenize, normalize, stemming, stop words removal

# set retrival
1. using grep - not efficient  
2. boolean model - answer a boolean query 
3. inverted index 
    steps : doc -> tokenize (token, doc id) -> stemming -> sort tokens ->  form { token : {freq, [postings sorted by doc id]} }
    query processing : and- merge, or, not (Process terms in increasing document frequency)

eval  :
P = P(relevant | retrieved)
R =  P(retrieved | relevant)
F1 = 2PR / P+R


# ranked retrieval
eval : 

# bag of words model

# vector space models
cos. similarity
Simple Matching
Dice’s Coefficient
Jaccard’s Coefficient
euclid. distance
length norm.
"""

import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
stop_words = set(stopwords.words("english"))
stemmer = PorterStemmer()

doc = """
Muad'Dib learned rapidly because his first training was in how to learn.
And the first lesson of all was the basic trust that he could learn.
It's shocking to find how many people do not believe they can learn,
and how many more believe learning to be difficult."""

def tokenize(doc):
  return word_tokenize(doc)

def remove_stop_words(l):
  filtered_list = []
  for word in l:
    if word.casefold() not in stop_words:
      filtered_list.append(word)
  return filtered_list

def stem(l):
  return [stemmer.stem(word) for word in l]

tokenized_doc = tokenize(doc)
filtered_doc = remove_stop_words(tokenized_doc)
stemmed_doc = stem(filtered_doc)
for i in range(len(stemmed_doc)):
  print(filtered_doc[i], "-->", stemmed_doc[i])

f = open("file.txt", "r")
data = f.read()
#print(data)


data=data.lower()

#remove punctuations
punc = '''!()-[]{};:'"\, <>./?@#$%^&*_~'''
for ele in data:
	if ele in punc:
		data = data.replace(ele, " ")


docs = data.split("\n")

#tokenize and stemming
ps = PorterStemmer()
for i in range(len(docs)):
    temp = word_tokenize(docs[i])
    #temp = [ps.stem(i) for i in temp]
    
    docs[i] = [i for i in temp if i not in stop_words]
    terms += docs[i]
terms = sorted(terms)
#print(docs)
print(terms)